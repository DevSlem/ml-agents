{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Agents Q-Learning with GridWorld\n",
    "\n",
    "Q-Learning 모델로 ML-Agents의 GridWorld 환경에서 학습  \n",
    "[Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_19_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)을 클론 코딩함으로써 Unity ML-Agents python low level api와 Deep Reinforcement Learning (DRL)에 대한 실습을 진행함.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/images/gridworld.png?raw=true\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GridWorld Environment with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the  GridWorld Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GridWorld](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Learning-Environment-Examples.md#gridworld) Environment는 간단한 Unity Visual environment이다. Agent는 파란색 사각형이며 3x3 grid내에서 red `x`를 피하면서 green `+`에 도달하는것을 목표로 한다.\n",
    "\n",
    "observation은 image로 grid의 위에서 카메라에 의해 획득된다.\n",
    "\n",
    "Action은 5개 중 하나이다.\n",
    "\n",
    "* Do not move\n",
    "* Move up\n",
    "* Move down\n",
    "* Move right\n",
    "* Move left\n",
    "\n",
    "Agent는 green `+`에 도달하면 1.0의 reward를 획득한다. red `x`에 도달 시 -1의 패널티를 획득한다. 또한 각 step마다 -0.01의 패널티가 부여된다.\n",
    "\n",
    "> **Note** There are 9 Agents, each in their own grid, at once in the environment. This allows for faster data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Double Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Q-Learning 알고리즘을 구현하기 위해 먼저 [pytorch](https://pytorch.org/)를 사용해 Deep Q Network(DQN)을 만들었다.  \n",
    "아래는 입력으로 이미지를 처리하는 매우 간단한 DQN인 `VisualQNetwork` class이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "    \"\"\"image를 학습하는 매우 간단한 visual neural\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: Tuple[int, int, int], encoding_size: int, output_size: int):\n",
    "        \"\"\"image batch (3 dimensional tensors)를 입력으로 사용하는 neural network를 생성한다.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): channel, height, width\n",
    "            encoding_size (int): fully connected layer의 encoding size\n",
    "            output_size (int): ouput size\n",
    "        \"\"\"\n",
    "        \n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        \n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        initial_channels = input_shape[0]\n",
    "        conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "        \n",
    "        self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32 # flatten된 conv2 ouput tensor의 size: height * width * out_channels\n",
    "        self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "        self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        \n",
    "    def forward(self, visual_obs: torch.Tensor):\n",
    "        conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "        conv_2 = torch.relu(self.conv2(conv_1))\n",
    "        hidden = self.dense1(conv_2.reshape([-1, self.final_flat])) # flatten and input to the fully connected layer\n",
    "        hidden = torch.relu(hidden) # activation function\n",
    "        hidden = self.dense2(hidden)\n",
    "        return hidden\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_output_shape(h_w: Tuple[int, int], kernel_size: int = 1, stride: int = 1, pad: int = 0, dilation: int = 1):\n",
    "        \"\"\"convolution layer의 출력의 height과 width를 반환한다.\"\"\"\n",
    "        \n",
    "        h = floor(\n",
    "            ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "        )\n",
    "        w = floor(\n",
    "            ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "        )\n",
    "        return h, w\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning을 학습시키는데 사용할 data를 저장하기 위한 data type을 정의한다. ReplayBuffer에 쓰인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    \"\"\"Agent transition data를 포함하는 experience\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "    \n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer` class를 정의한다. `Trainer` class는 policy를 따르는 environment로부터 data를 모은 뒤 2개의 DQN을 학습해 Double Q-learning을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VisualQNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31920\\1150467848.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31920\\1150467848.py\u001b[0m in \u001b[0;36mTrainer\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_trajectories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBaseEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_net\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mVisualQNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \"\"\"Q-Network로부터 획득된 policy와 함께 주어진 Unity Environment를 실행해 획득된 experience들의 buffer를 생성한다.\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VisualQNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "class DoubleDQNTrainer:\n",
    "    \n",
    "    def __init__(self, env: BaseEnv, default_q_net: VisualQNetwork, target_q_net: VisualQNetwork, optimzier: torch.optim):\n",
    "        \"\"\"Double DQN Trainer class\n",
    "\n",
    "        Args:\n",
    "            env (BaseEnv): The UnityEnvironment used.\n",
    "            default_q_net (VisualQNetwork): The default Q-Network used to collect the data.\n",
    "            target_q_net (VisualQNetwork): The target Q-Network used to update q-values of the default Q-Network\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.default_q_net = default_q_net\n",
    "        self.target_q_net = target_q_net\n",
    "        self.optimizer = optimzier\n",
    "    \n",
    "    def generate_trajectories(self, buffer_size: int, epsilon: float):\n",
    "        \"\"\"Q-Network로부터 획득된 policy와 함께 주어진 Unity Environment를 실행해 획득된 experience들의 buffer를 생성한다.\n",
    "\n",
    "        Args:\n",
    "            buffer_size (int): The minimum size of the buffer this method will return.\n",
    "            epsilon (float): Will add a random normal variable with standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "            a Tuple containing the created buffer and the average cumulative the Agents obtained.\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer: Buffer = []\n",
    "        \n",
    "        # Reset the environment\n",
    "        self.env.reset()\n",
    "        # Environment의 Behavior Name을 저장함\n",
    "        behavior_name = list(self.env.behavior_specs)[0]\n",
    "        # Enviornment의 Behavior Specs를 저장함\n",
    "        spec = self.env.behavior_specs[behavior_name]\n",
    "        \n",
    "        # AgentID에서 Trajectories로의 Mapping을 생성. 각 Agent에 대한 trajectories를 생성하기 위해서임.\n",
    "        dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "        # AgentId에서 Agent의 last observation으로의 Mapping을 생성.\n",
    "        dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # AgentID에서 Agent의 last action으로의 Mapping을 생성.\n",
    "        dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # AgentID에서 cumulative reward로의 Mapping을 생성.\n",
    "        dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "        # 지금까지 획득된 comulative reward를 저장하는 list 생성.\n",
    "        cumulative_rewards: List[float] = []\n",
    "        \n",
    "        # buffer에 데이터가 충분해질때까지 반복\n",
    "        while len(buffer) < buffer_size:\n",
    "            # Agent의 Decision Step과 Terminal Step 획득\n",
    "            decision_steps, terminal_steps = self.env.get_steps(behavior_name)\n",
    "            \n",
    "            # tensor를 NHWC에서 NCHW 형태로 변경. 즉, 이미지 tensor 채널을 PyTorch 입력에 맞게 변경함.\n",
    "            order = (0, 3, 1, 2)\n",
    "            decision_steps.obs = [np.transpose(obs, order) for obs in decision_steps.obs]\n",
    "            terminal_steps.obs = [np.transpose(obs, order) for obs in terminal_steps.obs]\n",
    "            \n",
    "            # Terminal Step을 가진 모든 Agent에 대해 반복\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                # Agent가 terminated됬기 때문에 last experience를 생성\n",
    "                last_experience = Experience(\n",
    "                    obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "                    reward=terminal_steps[agent_id_terminated].reward,\n",
    "                    done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "                    action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "                    next_obs=terminal_steps[agent_id_terminated].obs[0]\n",
    "                )\n",
    "                \n",
    "                # trajectory가 끝났기 때문에 agent의 last observation과 action을 제거함\n",
    "                dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "                dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "                # cumulative reward를 기록함\n",
    "                cumulative_reward = dict_cumulative_reward_from_agent.pop(agent_id_terminated) + terminal_steps[agent_id_terminated].reward\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                # Trajectory와 last experience를 buffer에 추가\n",
    "                buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "                buffer.append(last_experience)\n",
    "            \n",
    "            # Decision Step을 가진 모든 Agent에 대해\n",
    "            for agent_id_decisions in decision_steps:\n",
    "                # Agent가 Trajectory가 없다면 생성\n",
    "                if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "                    dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "                    \n",
    "                # decision을 요청하는 Agent가 \"last observation\"을 가지고 있다면\n",
    "                if agent_id_decisions in dict_last_obs_from_agent:\n",
    "                    # last observation으로부터의 Experience와 Decision Step을 생성\n",
    "                    exp = Experience(\n",
    "                        obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "                        reward=decision_steps[agent_id_decisions].reward,\n",
    "                        done=False,\n",
    "                        action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "                        next_obs=decision_steps[agent_id_decisions].obs[0]\n",
    "                    )\n",
    "                    # Agent의 Trajectory와 cumulative reward를 Update\n",
    "                    dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] += decision_steps[agent_id_decisions].reward\n",
    "                    \n",
    "                # 새로운 \"last observation\"을 저장함\n",
    "                dict_last_obs_from_agent[agent_id_decisions] = decision_steps[agent_id_decisions].obs[0]\n",
    "                \n",
    "            # decision을 요청한 모든 Agent에 대한 action을 생성\n",
    "            # observation에 대한 각 action value를 계산\n",
    "            q_values = self.default_q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "            # noise 추가\n",
    "            q_values += epsilon * np.random.randn(q_values.shape[0], q_values.shape[1]).astype(np.float32)\n",
    "            # best action 선택\n",
    "            actions = np.argmax(q_values, axis=1)\n",
    "            actions.resize((len(decision_steps), 1))\n",
    "            # 선택된 action들을 저장\n",
    "            for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "                dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "            \n",
    "            # environment 내 action들을 설정\n",
    "            # Unity Environments는 ActionTuple instance를 사용함\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(actions)\n",
    "            self.env.set_actions(behavior_name, action_tuple)\n",
    "            # 다음 step으로 진행\n",
    "            self.env.step()\n",
    "        return buffer, np.mean(cumulative_rewards)\n",
    "    \n",
    "\n",
    "    def update_double_dqn(self, buffer: Buffer, action_size: int):\n",
    "        \"\"\"제공된 optimizer와 buffer로 Q-Network를 update함.\"\"\"\n",
    "        \n",
    "        BATCH_SIZE = 1000\n",
    "        NUM_EPOCH = 3\n",
    "        GAMMA = 0.9\n",
    "                \n",
    "        batch_size = min(len(buffer), BATCH_SIZE)\n",
    "        random.shuffle(buffer)\n",
    "        # buffer를 batch size 단위로 split함.\n",
    "        batches = [buffer[batch_size * start : batch_size * (start + 1)] for start in range(int(len(buffer) / batch_size))]\n",
    "        \n",
    "        for _ in range(NUM_EPOCH):\n",
    "            for batch in batches:\n",
    "                # network에 입력될 Tensor를 생성\n",
    "                obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "                reward = torch.from_numpy(np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1))\n",
    "                done = torch.from_numpy(np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1))\n",
    "                action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "                next_obs = torch.from_numpy(np.stack(ex.next_obs for ex in batch))\n",
    "                \n",
    "                # Q-Network를 update하기 위해 Double Q-learning 알고리즘 사용\n",
    "                \n",
    "                q_values = self.default_q_net(obs)\n",
    "                next_q_values = self.default_q_net(next_obs)\n",
    "                next_q_target_values = self.target_q_net(next_obs)\n",
    "                \n",
    "                prediction = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # mask = torch.zeros((len(batch), action_size))\n",
    "                # mask.scatter_(1, action, 1)\n",
    "                # prediction = torch.sum(q_net(obs) * mask, dim=1, keepdim=True)\n",
    "                \n",
    "                target = reward + (1.0 - done) * GAMMA * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
    "\n",
    "                criterion = torch.nn.MSELoss()\n",
    "                loss = criterion(prediction, target)\n",
    "                \n",
    "                # backpropagation 실행\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld environment created.\n",
      "Training step: 1 \treward: -0.7777777579095628\n",
      "Training step: 2 \treward: -0.7788888689958386\n",
      "Training step: 3 \treward: -0.7788888689958386\n",
      "Training step: 4 \treward: -0.9999999821186065\n",
      "Training step: 5 \treward: -0.7799999800821146\n",
      "Training step: 6 \treward: -0.9999999776482582\n",
      "Training step: 7 \treward: -0.8029999820515513\n",
      "Training step: 8 \treward: -0.7777777579095628\n",
      "Training step: 9 \treward: -0.9999999776482582\n",
      "Training step: 10 \treward: -0.999999980131785\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------\n",
    "# Unity Environment가 닫혀있지 않았을 경우를 위한 처리\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# CridWorl Environment 생성\n",
    "env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "num_actions = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "try:\n",
    "    # Q-Network 생성\n",
    "    qnet = VisualQNetwork((3, 64, 84), 126, num_actions)\n",
    "    \n",
    "    experiences: Buffer = []\n",
    "    optim = torch.optim.Adam(qnet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    cumulatvie_rewards: List[float] = []\n",
    "    \n",
    "    NUM_TRAINING_STEPS = 10 # The number of training steps that will be performed\n",
    "    NUM_NEW_EXP = 1000 # The number of experiences to collect per training step\n",
    "    BUFFER_SIZE = 10000 # The maximum size of the Buffer\n",
    "    \n",
    "    for n in range(NUM_TRAINING_STEPS):\n",
    "        new_exp, _ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "        \n",
    "        if len(experiences) > BUFFER_SIZE:\n",
    "            experiences = experiences[:BUFFER_SIZE]\n",
    "        experiences.extend(new_exp)\n",
    "        Trainer.update_q_net(qnet, optim, experiences, num_actions)\n",
    "        _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "        cumulatvie_rewards.append(rewards)\n",
    "        print(f\"Training step: {n + 1} \\treward: {rewards}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted, continue to next cell to save to save the model.\")\n",
    "finally:\n",
    "    env.close()\n",
    "    \n",
    "# Show the training graph\n",
    "try:\n",
    "    plt.plot(range(NUM_TRAINING_STEPS), cumulatvie_rewards)\n",
    "except ValueError:\n",
    "    print(\"\\nPlot failed on interrupted training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export PyTorch Model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity ML-Agents의 inference에 사용할 model을 추출한다. 이를 위해 다음과 같은 추가적인 tensor가 필요하다.\n",
    "\n",
    "* All models need version_number\n",
    "* All models need memory_size\n",
    "* Models with **continuous outputs** need continuous_action_output_shape\n",
    "* Models with **discrete outputs** need discrete_action_output_shape and an additional mask input that matches the shape of the discrete outputs\n",
    "* The mask input must be connected to the outputs or it will be pruned on export, if mask values aren't being set they will be 1, so multiplying the discrete outputs by the mask will have no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, qnet: VisualQNetwork, discrete_ouput_sizes: List[int]):\n",
    "        \"\"\"\n",
    "        runtime inference에 의해 요구되는 여분의 상수들과 \n",
    "        더미 mask input들을 추가한 VisualQNetwork를 Wraping한다.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(WrapperNet, self).__init__()\n",
    "        self.qnet = qnet\n",
    "        \n",
    "        # version_number\n",
    "        #   MLAgents1_0 = 2\n",
    "        #   MLAgents2_0 = 3\n",
    "        version_number = torch.Tensor([3])\n",
    "        self.version_number = Parameter(version_number, requires_grad=False)\n",
    "        \n",
    "        # memory_size\n",
    "        # TODO: document case where memory is not zero.\n",
    "        memory_size = torch.Tensor([0])\n",
    "        self.memory_size = Parameter(memory_size, requires_grad=False)\n",
    "        \n",
    "        # discrete_action_ouput_shape\n",
    "        output_shape = torch.Tensor([discrete_ouput_sizes])\n",
    "        self.discrete_shape = Parameter(output_shape, requires_grad=False)\n",
    "        \n",
    "    # discrete action이라면 input으로 같은 shape의 mask tensor를 받는다.\n",
    "    def forward(self, visual_obs: torch.Tensor, mask: torch.Tensor):\n",
    "        qnet_result = self.qnet(visual_obs)\n",
    "        # 제거되는 현상을 막기 위해 mask를 연결\n",
    "        # WriteDiscreteActionMask() 내에서 SetActionMask()를 호출하지 않을 경우 mask value는 1\n",
    "        qnet_result = torch.mul(qnet_result, mask)\n",
    "        action = torch.argmax(qnet_result, dim=1, keepdim=True)\n",
    "        return [action], self.discrete_shape, self.version_number, self.memory_size\n",
    "    \n",
    "torch.onnx.export(\n",
    "    WrapperNet(qnet, [num_actions]),\n",
    "    (torch.tensor([experiences[0].obs]), torch.ones(1, num_actions)),\n",
    "    \"GridWorld.onnx\",\n",
    "    opset_version=9,\n",
    "    input_names=[\"obs_0\", \"action_masks\"],\n",
    "    output_names=[\"discrete_actions\", \"discrete_action_ouput_shape\", \"version_number\", \"memory_size\"],\n",
    "    dynamic_axes={\n",
    "        \"obs_0\": {0: \"batch\"},\n",
    "        \"action_masks\": {0: \"batch\"},\n",
    "        \"discrete_actions\": {0: \"batch\"},\n",
    "        \"discrete_action_ouput_shape\": {0: \"batch\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_19_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc276fecccee84e45895f749ec0e13ea9f4c828862c99e3c5f7e443fba7710e4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ml-agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
