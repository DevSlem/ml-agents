{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Agents Q-Learning with GridWorld\n",
    "\n",
    "Q-Learning 모델로 ML-Agents의 GridWorld 환경에서 학습  \n",
    "[Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_19_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)을 클론 코딩함으로써 Unity ML-Agents python low level api와 Deep Reinforcement Learning (DRL)에 대한 실습을 진행함.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/images/gridworld.png?raw=true\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GridWorld Environment with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the  GridWorld Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GridWorld](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Learning-Environment-Examples.md#gridworld) Environment는 간단한 Unity Visual environment이다. Agent는 파란색 사각형이며 3x3 grid내에서 red `x`를 피하면서 green `+`에 도달하는것을 목표로 한다.\n",
    "\n",
    "observation은 image로 grid의 위에서 카메라에 의해 획득된다.\n",
    "\n",
    "Action은 5개 중 하나이다.\n",
    "\n",
    "* Do not move\n",
    "* Move up\n",
    "* Move down\n",
    "* Move right\n",
    "* Move left\n",
    "\n",
    "Agent는 green `+`에 도달하면 1.0의 reward를 획득한다. red `x`에 도달 시 -1의 패널티를 획득한다. 또한 각 step마다 -0.01의 패널티가 부여된다.\n",
    "\n",
    "> **Note** There are 9 Agents, each in their own grid, at once in the environment. This allows for faster data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 간단한 Q-Learning 알고리즘으로 [pytorch](https://pytorch.org/)를 사용하였다.  \n",
    "\n",
    "아래는 매우 간단한 신경망이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "    \"\"\"image를 학습하는 매우 간단한 visual neural\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: Tuple[int, int, int], encoding_size: int, output_size: int):\n",
    "        \"\"\"image batch (3 dimensional tensors)를 입력으로 사용하는 neural network를 생성한다.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): channel, height, width\n",
    "            encoding_size (int): fully connected layer의 encoding size\n",
    "            output_size (int): ouput size\n",
    "        \"\"\"\n",
    "        \n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        \n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        initial_channels = input_shape[0]\n",
    "        conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "        \n",
    "        self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32 # flatten된 conv2 ouput tensor의 size: height * width * out_channels\n",
    "        self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "        self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        \n",
    "    def forward(self, visual_obs: torch.Tensor):\n",
    "        conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "        conv_2 = torch.relu(self.conv2(conv_1))\n",
    "        hidden = self.dense1(conv_2.reshape([-1, self.final_flat])) # flatten and input to the fully connected layer\n",
    "        hidden = torch.relu(hidden) # activation function\n",
    "        hidden = self.dense2(hidden)\n",
    "        return hidden\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_output_shape(h_w: Tuple[int, int], kernel_size: int = 1, stride: int = 1, pad: int = 0, dilation: int = 1):\n",
    "        \"\"\"convolution layer의 출력의 height과 width를 반환한다.\"\"\"\n",
    "        \n",
    "        h = floor(\n",
    "            ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "        )\n",
    "        w = floor(\n",
    "            ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "        )\n",
    "        return h, w\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning을 학습시키는데 사용할 data를 저장하기 위한 data type 정의. ReplayBuffer에 쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    \"\"\"Agent transition data를 포함하는 experience\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "    \n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer class를 정의함. trainer class는 policy를 따르는 environment로부터 data를 모은 뒤 Q-Network를 학습함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Trainer class 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_19_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc276fecccee84e45895f749ec0e13ea9f4c828862c99e3c5f7e443fba7710e4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ml-agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
