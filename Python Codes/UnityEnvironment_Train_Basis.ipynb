{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Agents Q-Learning with GridWorld\n",
    "\n",
    "Q-Learning 모델로 ML-Agents의 GridWorld 환경에서 학습  \n",
    "[Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_19_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)을 클론 코딩함으로써 Unity ML-Agents python low level api와 Deep Reinforcement Learning (DRL)에 대한 실습을 진행함.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/images/gridworld.png?raw=true\" align=\"middle\" width=\"435\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GridWorld Environment with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the  GridWorld Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GridWorld](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Learning-Environment-Examples.md#gridworld) Environment는 간단한 Unity Visual environment이다. Agent는 파란색 사각형이며 3x3 grid내에서 red `x`를 피하면서 green `+`에 도달하는것을 목표로 한다.\n",
    "\n",
    "observation은 image로 grid의 위에서 카메라에 의해 획득된다.\n",
    "\n",
    "Action은 5개 중 하나이다.\n",
    "\n",
    "* Do not move\n",
    "* Move up\n",
    "* Move down\n",
    "* Move right\n",
    "* Move left\n",
    "\n",
    "Agent는 green `+`에 도달하면 1.0의 reward를 획득한다. red `x`에 도달 시 -1의 패널티를 획득한다. 또한 각 step마다 -0.01의 패널티가 부여된다.\n",
    "\n",
    "> **Note** There are 9 Agents, each in their own grid, at once in the environment. This allows for faster data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 간단한 Q-Learning 알고리즘으로 [pytorch](https://pytorch.org/)를 사용하였다.  \n",
    "\n",
    "아래는 매우 간단한 신경망이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "    \"\"\"image를 학습하는 매우 간단한 visual neural\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: Tuple[int, int, int], encoding_size: int, output_size: int):\n",
    "        \"\"\"image batch (3 dimensional tensors)를 입력으로 사용하는 neural network를 생성한다.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): channel, height, width\n",
    "            encoding_size (int): fully connected layer의 encoding size\n",
    "            output_size (int): ouput size\n",
    "        \"\"\"\n",
    "        \n",
    "        super(VisualQNetwork, self).__init__()\n",
    "        \n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        initial_channels = input_shape[0]\n",
    "        conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "        \n",
    "        self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32 # flatten된 conv2 ouput tensor의 size: height * width * out_channels\n",
    "        self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "        self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        \n",
    "    def forward(self, visual_obs: torch.Tensor):\n",
    "        conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "        conv_2 = torch.relu(self.conv2(conv_1))\n",
    "        hidden = self.dense1(conv_2.reshape([-1, self.final_flat])) # flatten and input to the fully connected layer\n",
    "        hidden = torch.relu(hidden) # activation function\n",
    "        hidden = self.dense2(hidden)\n",
    "        return hidden\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_output_shape(h_w: Tuple[int, int], kernel_size: int = 1, stride: int = 1, pad: int = 0, dilation: int = 1):\n",
    "        \"\"\"convolution layer의 출력의 height과 width를 반환한다.\"\"\"\n",
    "        \n",
    "        h = floor(\n",
    "            ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "        )\n",
    "        w = floor(\n",
    "            ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "        )\n",
    "        return h, w\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning을 학습시키는데 사용할 data를 저장하기 위한 data type 정의. ReplayBuffer에 쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    \"\"\"Agent transition data를 포함하는 experience\"\"\"\n",
    "    \n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "    \n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer class를 정의함. trainer class는 policy를 따르는 environment로부터 data를 모은 뒤 Q-Network를 학습함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_trajectories(env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float):\n",
    "        \"\"\"Q-Network로부터 획득된 policy와 함께 주어진 Unity Environment를 실행해 획득된 experience들의 buffer를 생성한다.\n",
    "\n",
    "        Args:\n",
    "            env (BaseEnv): The UnityEnvironment used.\n",
    "            q_net (VisualQNetwork): The Q-Network used to collect the data.\n",
    "            buffer_size (int): The minimum size of the buffer this method will return.\n",
    "            epsilon (float): Will add a random normal variable with standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "            a Tuple containing the created buffer and the average cumulative the Agents obtained.\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer: Buffer = []\n",
    "        \n",
    "        # Reset the environment\n",
    "        env.reset()\n",
    "        # Environment의 Behavior Name을 저장함\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        # Enviornment의 Behavior Specs를 저장함\n",
    "        spec = env.behavior_specs[behavior_name]\n",
    "        \n",
    "        # AgentID에서 Trajectories로의 Mapping을 생성. 각 Agent에 대한 trajectories를 생성하기 위해서임.\n",
    "        dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "        # AgentId에서 Agent의 last observation으로의 Mapping을 생성.\n",
    "        dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # AgentID에서 Agent의 last action으로의 Mapping을 생성.\n",
    "        dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "        # AgentID에서 cumulative reward로의 Mapping을 생성.\n",
    "        dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "        # 지금까지 획득된 comulative reward를 저장하는 list 생성.\n",
    "        cumulative_rewards: List[float] = []\n",
    "        \n",
    "        # buffer에 데이터가 충분해질때까지 반복\n",
    "        while len(buffer) < buffer_size:\n",
    "            # Agent의 Decision Step과 Terminal Step 획득\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            \n",
    "            # tensor를 NHWC에서 NCHW 형태로 변경. 즉, 이미지 tensor 채널을 PyTorch 입력에 맞게 변경함.\n",
    "            order = (0, 3, 1, 2)\n",
    "            decision_steps.obs = [np.transpose(obs, order) for obs in decision_steps.obs]\n",
    "            decision_steps.obs = [np.transpose(obs, order) for obs in terminal_steps.obs]\n",
    "            \n",
    "            # Terminal Step을 가진 모든 Agent에 대해 반복\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                # Agent가 terminated됬기 때문에 last experience를 생성\n",
    "                last_experience = Experience(\n",
    "                    obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "                    reward=terminal_steps[agent_id_terminated].reward,\n",
    "                    done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "                    action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "                    next_obs=terminal_steps[agent_id_terminated].obs[0]\n",
    "                )\n",
    "                \n",
    "                # trajectory가 끝났기 때문에 agent의 last observation과 action을 제거함\n",
    "                dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "                dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "                # cumulative reward를 기록함\n",
    "                cumulative_reward = dict_cumulative_reward_from_agent.pop(agent_id_terminated) + last_experience.reward\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                # Trajectory와 last experience를 buffer에 추가\n",
    "                buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "                buffer.append(last_experience)\n",
    "            \n",
    "            # Decision Step에 대한 코드 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Q-Learning with a UnityEnvironment](https://colab.research.google.com/github/Unity-Technologies/ml-agents/blob/release_19_docs/colab/Colab_UnityEnvironment_2_Train.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc276fecccee84e45895f749ec0e13ea9f4c828862c99e3c5f7e443fba7710e4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ml-agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
